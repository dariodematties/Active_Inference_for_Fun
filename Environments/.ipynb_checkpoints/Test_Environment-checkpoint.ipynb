{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff4b308b-8f66-4aaf-b436-53368eebee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gridworld_env import GridWorld\n",
    "from ai_agent_factory import build_gridworld_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cdaed35-a3ba-405c-be8e-0b5119f985c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "A matrix must be a numpy array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m GridWorld(n_rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, n_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, reward_pos\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m), punish_pos\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m4\u001b[39m), start_pos\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m), render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m agent, model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_gridworld_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpunish_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Active_Inference_for_Fun/Environments/ai_agent_factory.py:71\u001b[0m, in \u001b[0;36mbuild_gridworld_agent\u001b[0;34m(n_rows, n_cols, reward_pos, punish_pos, start_pos, c_reward, c_punish, policy_len, gamma, action_selection)\u001b[0m\n\u001b[1;32m     68\u001b[0m C_list \u001b[38;5;241m=\u001b[39m [C]\n\u001b[1;32m     69\u001b[0m D_list \u001b[38;5;241m=\u001b[39m [D]\n\u001b[0;32m---> 71\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mAgent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mA_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mB_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mC_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mD_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_controls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# one control factor (movement)\u001b[39;49;00m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# short horizon to start\u001b[39;49;00m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43minference_horizon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# policy precision\u001b[39;49;00m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction_selection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_selection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# \"stochastic\" or \"deterministic\"\u001b[39;49;00m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_utility\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_states_info_gain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_param_info_gain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m model \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m: A_list, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m: B_list, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m: C_list, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m\"\u001b[39m: D_list,\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m: reward_idx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpunish_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m: punish_idx,\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos_to_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m: pos_to_idx,\n\u001b[1;32m     90\u001b[0m }\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m agent, model\n",
      "File \u001b[0;32m~/python_venvs/pymdp/lib64/python3.12/site-packages/pymdp/agent.py:88\u001b[0m, in \u001b[0;36mAgent.__init__\u001b[0;34m(self, A, B, C, D, E, pA, pB, pD, num_controls, policy_len, inference_horizon, control_fac_idx, policies, gamma, alpha, use_utility, use_states_info_gain, use_param_info_gain, action_selection, sampling_mode, inference_algo, inference_params, modalities_to_learn, lr_pA, factors_to_learn, lr_pB, lr_pD, use_BMA, policy_sep_prior, save_belief_hist)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Initialise observation model (A matrices)\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA matrix must be a numpy array\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mto_obj_array(A)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mis_normalized(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA matrix is not normalized (i.e. A.sum(axis = 0) must all equal 1.0)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: A matrix must be a numpy array"
     ]
    }
   ],
   "source": [
    "env = GridWorld(n_rows=4, n_cols=5, reward_pos=(3,4), punish_pos=(0,4), start_pos=(0,0), render_mode=None)\n",
    "agent, model = build_gridworld_agent(4, 5, reward_pos=(3,4), punish_pos=(0,4), start_pos=(0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3920d469-a843-4644-b1db-849c0fc434af",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset(seed=0)\n",
    "for _ in range(5):\n",
    "    agent.infer_states(obs)         # single modality: pass scalar index (not list)\n",
    "    agent.infer_policies()\n",
    "    action = int(agent.sample_action())\n",
    "    obs, r, term, trunc, _ = env.step(action)\n",
    "    if term or trunc:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e826e2a0-13ec-4266-beb3-e25b23c21339",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
